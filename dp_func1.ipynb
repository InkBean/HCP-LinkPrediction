{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94057535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import copy\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b44d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = loadmat(os.getcwd() + '/' + 'sub_id.mat')\n",
    "subid_arr = subj['Subject']\n",
    "\n",
    "subid_list = []\n",
    "for subid in subid_arr:\n",
    "    subid_list.append(subid[0])\n",
    "\n",
    "struct_p = \"structures.mat\"\n",
    "dist_p = \"distance.csv\"\n",
    "cur_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3887176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_add_flatten(dti_matrix):\n",
    "    ######## REMEMBER TO TAKE THE TANSPOSE, ADD IT TO THE ORIGINAL ONE THEN DIVIDE BY 2\n",
    "    dti_transposed = dti_matrix.transpose()\n",
    "    dti = (dti_matrix + dti_transposed) / 2\n",
    "    dti_arr = dti.flatten()  ## FLATTEN IT\n",
    "\n",
    "    return dti_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4f1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_structure(PATH, head):\n",
    "    if head:\n",
    "        structure = pd.read_csv(PATH, header=0)\n",
    "    else:\n",
    "        structure = pd.read_csv(PATH, header=None)\n",
    "    \n",
    "    structure_arr = structure.to_numpy()\n",
    "    \n",
    "    thickness = structure_arr[:,0]    \n",
    "\n",
    "    myelination = structure_arr[:, 1]\n",
    "    curvature = structure_arr[:, 2]\n",
    "    sulcus_depth = structure_arr[:, 3]\n",
    "    \n",
    "    thickness_pairs = []\n",
    "    for i in thickness:\n",
    "        for j in thickness:\n",
    "            diff = abs(i - j)\n",
    "            thickness_pairs.append(diff)\n",
    "\n",
    "    T_pairs = np.array(thickness_pairs)\n",
    "    T_pairs = (T_pairs - min(T_pairs)) / (max(T_pairs) - min(T_pairs))\n",
    "    T_pairs = 1 - T_pairs\n",
    "\n",
    "    myelination_pairs = []\n",
    "    for i in myelination:\n",
    "        for j in myelination:\n",
    "            diff = abs(i - j)\n",
    "            myelination_pairs.append(diff)\n",
    "\n",
    "    M_pairs = np.array(myelination_pairs)\n",
    "    M_pairs = (M_pairs - min(M_pairs)) / (max(M_pairs) - min(M_pairs))\n",
    "    M_pairs = 1 - M_pairs\n",
    "\n",
    "    curvature_pairs = []\n",
    "    for i in curvature:\n",
    "        for j in curvature:\n",
    "            diff = abs(i - j)\n",
    "            curvature_pairs.append(diff)\n",
    "    \n",
    "    C_pairs = np.array(curvature_pairs)\n",
    "    C_pairs = (C_pairs - min(C_pairs)) / (max(C_pairs) - min(C_pairs))\n",
    "    C_pairs = 1 - C_pairs\n",
    "\n",
    "    #\n",
    "    sulcus_pairs = []\n",
    "    for i in sulcus_depth:\n",
    "        for j in sulcus_depth:\n",
    "            diff = abs(i - j)\n",
    "            sulcus_pairs.append(diff)\n",
    "\n",
    "    S_pairs = np.array(sulcus_pairs)\n",
    "    S_pairs = (S_pairs - min(S_pairs)) / (max(S_pairs) - min(S_pairs))\n",
    "    S_pairs = 1 - S_pairs\n",
    "    \n",
    "    \n",
    "    X_4F = np.column_stack((T_pairs, M_pairs))\n",
    "    X_4F = np.column_stack((X_4F, C_pairs))\n",
    "    X_4F = np.column_stack((X_4F, S_pairs))\n",
    "    \n",
    "    return X_4F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2839e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(arr, percentile):\n",
    "    arr_copy = copy.deepcopy(arr)\n",
    "    sorted_arr = np.sort(arr_copy)\n",
    "    idx = int(np.floor(len(sorted_arr) * percentile))\n",
    "    if percentile == 1:\n",
    "        idx = len(arr) - 1\n",
    "    threshold = sorted_arr[idx]\n",
    "#     print(idx)\n",
    "    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200f69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattened_to_matrix(flattened):\n",
    "    side_len = int(len(flattened) ** 0.5)\n",
    "    matrix = np.zeros((side_len, side_len))\n",
    "    print(\"side length: \" + str(side_len))\n",
    "#     print(\"--\")\n",
    "    for i in range(side_len):\n",
    "        for j in range(side_len):\n",
    "            idx = i * side_len + j\n",
    "#             if idx == 32400:\n",
    "#                 print(\"wtf\")\n",
    "#                 break\n",
    "            matrix[i, j] = flattened[idx]\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1080b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE TOPOLOGICAL SIMILARITY BASED ON ADJACENCY MATRIX\n",
    "# THE EQUATION USED IS \n",
    "#                      Tij = common neighbors of i&j / neighbors of i\n",
    "\n",
    "def to_topo(adjacency_matrix):\n",
    "\n",
    "    connection_list = []\n",
    "    \n",
    "    side_len = len(adjacency_matrix[0])\n",
    "    print(side_len)\n",
    "\n",
    "    for i in range(side_len):\n",
    "        nonzero_indices = list(np.nonzero(adjacency_matrix[i]))\n",
    "        connection_list.append(nonzero_indices)\n",
    "    \n",
    "    T = np.zeros((side_len, side_len))\n",
    "\n",
    "    for i in range(side_len):\n",
    "        a = connection_list[i][0]\n",
    "        a = set(a)\n",
    "    \n",
    "        for j in range(side_len):\n",
    "            b = connection_list[j][0]\n",
    "            b = set(b)\n",
    "        \n",
    "            common_connections = set(a).intersection(b)\n",
    "            union_connections = set(a).union(b)\n",
    "            \n",
    "            topo_similarity = 0\n",
    "            \n",
    "            if len(a) + len(b) != 0:\n",
    "                topo_similarity = sum(common_connections) / sum(union_connections)\n",
    "#                 topo_similarity = len(common_connections) / ((len(a) + len(b)) / 2)\n",
    "                \n",
    "#             print(topo_similarity)\n",
    "            T[i, j] = topo_similarity\n",
    "            \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ed4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_localStructures(PATH, sub_idx, subid_list, struct_matlab_path):\n",
    "    struct = loadmat(PATH + '/' + struct_matlab_path)\n",
    "    structures = struct['structures']\n",
    "    inter = structures[sub_idx,:,0:4]\n",
    "\n",
    "    structure_csv = \"localstructure_\" + str(subject_ID) + \".csv\"\n",
    "    a = np.savetxt(\"dp_intermediate/\" + structure_csv, inter, delimiter=\",\",fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9512702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backboneDTI_to_topo(flattened_dti):\n",
    "    AM = flattened_to_matrix(flattened_dti)\n",
    "    topo = to_topo(AM)\n",
    "    return topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f374955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeByCoverage(coverage):\n",
    "    idx_removed = []\n",
    "    if coverage != \"whole\":\n",
    "        side_len = 180\n",
    "        same_num = side_len ** 2\n",
    "        if coverage == \"left\":\n",
    "            for i in range(side_len):\n",
    "                for j in range(side_len):\n",
    "                    idx_removed.append(i * (2 * side_len) + side_len + j)\n",
    "            for k in range(same_num * 2):\n",
    "                idx_removed.append(k + same_num * 2)\n",
    "        elif coverage == \"right\":\n",
    "            for i in range(same_num * 2):\n",
    "                idx_removed.append(i)\n",
    "            for j in range(side_len):\n",
    "                for k in range(side_len):\n",
    "                    idx_removed.append(same_num * 2 + j * 360 + k)\n",
    "        elif coverage == \"contra\":\n",
    "            for i in range(2 * same_num):\n",
    "                idx_removed.append(2 * same_num + i)\n",
    "            for j in range(side_len):\n",
    "                for k in range(side_len):\n",
    "                    idx_removed.append(j * 360 + k)\n",
    "    \n",
    "    return idx_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d232db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dti_MatToCsv(mat_path, save_to, sub_id):\n",
    "    m = loadmat(mat_path)\n",
    "    dti = m['DTI']\n",
    "    dti = dti.flatten()\n",
    "    saved_name = save_to + \"/\" + str(sub_id) + \".csv\"\n",
    "    path = os.path.normpath(saved_name)\n",
    "    DTI_CSV = np.savetxt(path, dti, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03ed31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return list(l[i:i+n] for i in range(0, len(l), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9cfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bfce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(Y, is_log, beta):\n",
    "    if is_log == False:\n",
    "        Y = Y ** beta\n",
    "        print(\"beta:  \" +  str(scaling_params[\"beta\"])   + \"Y_max_beta: \" + str(Y.max()))\n",
    "    else:\n",
    "        Y = Y + 1e-09\n",
    "        Y = np.log(Y)\n",
    "        print(\"log:  \" + \"Y_max_log: \" + str(Y.max()))\n",
    "    Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "#     print(\"yo\")\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a707eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv_datataset_graph(sub_idx, subid_list, path_dict, scaling_params):\n",
    "    print(\"graph\")\n",
    "    subject_ID = subid_list[sub_idx]\n",
    "    PATH = path_dict[\"PATH\"]\n",
    "    output_folder = PATH\n",
    "\n",
    "    distance_matrix = pd.read_csv(path_dict[\"distance_path\"], header=None)\n",
    "    dist_arr = distance_matrix.to_numpy()\n",
    "    spatial_proximity = dist_arr.flatten()\n",
    "\n",
    "    spatial_proximity = (spatial_proximity - spatial_proximity.min()) / (spatial_proximity.max() - spatial_proximity.min()) \n",
    "\n",
    "    dti = pd.read_csv(PATH + \"/dti_csv/dti_mat/\" + str(subject_ID) + \"_MMP_matrix.csv\", header=None)\n",
    "    dti_np = dti.to_numpy()\n",
    "    dti = transpose_add_flatten(dti_np)\n",
    "    print(np.shape(dti))\n",
    "    \n",
    "    structure_csv = \"localStructures_\" + str(subject_ID) + \".csv\"\n",
    "    structures = read_structure(PATH + \"/structures_csv/\" + structure_csv, 0)\n",
    "    \n",
    "    if scaling_params[\"isAverage\"]:\n",
    "        dti_avg = pd.read_csv(PATH + \"/dti_998avg.csv\", header=None)\n",
    "        dti_avg = dti_avg.to_numpy().flatten()\n",
    "        dti_mat_avg = flattened_to_matrix(dti_avg)\n",
    "        dti = transpose_add_flatten(dti_mat_avg)\n",
    "        \n",
    "        structures = read_structure(PATH + \"/avg_structures.csv\", 0)\n",
    "        output_folder += \"/GraphAvgData/\"\n",
    "        print(\"avg!graph\")\n",
    "    else:\n",
    "        output_folder += \"/GraphData/\"\n",
    "\n",
    "    dti_copy = copy.deepcopy(dti)\n",
    "\n",
    "    if scaling_params[\"weigh_topo\"] == True: \n",
    "        # apply log scale on weight to compute topological similarity\n",
    "        dti_copy = dti_copy + 1e-09\n",
    "        dti_copy = np.log(dti_copy)\n",
    "        dti_copy = (dti_copy - dti_copy.min()) / (dti_copy.max() - dti_copy.min())\n",
    "    \n",
    "    max_scaleKnown = 1 / 10**(min(scaling_params[\"scales_known\"]))\n",
    "    min_scaleKnown = 1 / 10**(max(scaling_params[\"scales_known\"]))\n",
    "    max_scaleToPredict = 1 / 10**(min(scaling_params[\"scales_toPredict\"]))\n",
    "    min_scaleToPredict = 1 / 10**(max(scaling_params[\"scales_toPredict\"]))\n",
    "    if max(scaling_params[\"scales_toPredict\"]) == 8:\n",
    "        min_scaleToPredict = 0\n",
    "    \n",
    "    # Turn dti_copy to binary array with 1 for known links and 0 for unknown\n",
    "    dti_copy = np.where(dti_copy <= 10 * max_scaleKnown, dti_copy, 0)\n",
    "    dti_copy = np.where(dti_copy >= min_scaleKnown, dti_copy, 0)\n",
    "    if scaling_params[\"weigh_topo\"] == False:\n",
    "        dti_copy = np.where(dti_copy > 0, 1, 0)\n",
    "\n",
    "    adj_mat = flattened_to_matrix(dti_copy)\n",
    "    \n",
    "    output_folder += process_folderName(scaling_params)\n",
    "    print(output_folder)\n",
    "    if not scaling_params[\"isAverage\"]:\n",
    "        if path_dict[\"isTrain\"] == True:\n",
    "            output_folder += \"/Train/\"\n",
    "        else:\n",
    "            output_folder += \"/Test/\"\n",
    "    print(output_folder)\n",
    "    \n",
    "    dti_copy_y = copy.deepcopy(dti)\n",
    "    dti_copy_y = np.where(dti_copy_y <= 10 * max_scaleToPredict, dti_copy_y, 0)\n",
    "    dti_copy_y = np.where(dti_copy_y >= min_scaleToPredict, dti_copy_y, 0)\n",
    "    Y = dti_copy_y\n",
    "    \n",
    "    if scaling_params[\"apply_log\"] == False:\n",
    "        Y = Y ** scaling_params[\"beta\"]\n",
    "        print(\"beta:  \" +  str(scaling_params[\"beta\"])   + \"Y_max_beta: \" + str(Y.max()))\n",
    "    else:\n",
    "        Y = Y + 1e-09\n",
    "        Y = np.log(Y)\n",
    "        print(\"log:  \" + \"Y_max_log: \" + str(Y.max()))\n",
    "    \n",
    "    Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "    \n",
    "    toPred_AM = flattened_to_matrix(Y)\n",
    "    \n",
    "    output_path_x = output_folder + \"X/\"\n",
    "    output_path_y = output_folder + \"Y/\"\n",
    "    \n",
    "    if scaling_params[\"isAverage\"]:\n",
    "        output_path_x += \"AVG_known_AM.csv\"\n",
    "        output_path_y += \"AVG_pred_AM.csv\"\n",
    "    else:\n",
    "        output_path_x += str(subject_ID) + \"_known_AM.csv\"\n",
    "        output_path_y += str(subject_ID) + \"_pred_AM.csv\"\n",
    "\n",
    "    print(output_path_x)\n",
    "    print(output_path_y)\n",
    "    output_x = np.savetxt(output_path_x, adj_mat, delimiter=\",\",fmt = \"%s\")\n",
    "    output_y = np.savetxt(output_path_y, toPred_AM, delimiter=\",\",fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "272b72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv_dataset_avg(path_dict, scaling_params):\n",
    "    print(\"avg!\")\n",
    "    PATH = path_dict[\"PATH\"]\n",
    "    distance_matrix = pd.read_csv(path_dict[\"distance_path\"], header=None)\n",
    "\n",
    "    dist_arr = distance_matrix.to_numpy()\n",
    "    spatial_proximity = dist_arr.flatten()\n",
    "    \n",
    "    #################################### SHOULD NORMALIZE LATER !!!!!!!!!!!!! SUBJECT TO CHANGES\n",
    "    spatial_proximity = (spatial_proximity - spatial_proximity.min()) / (spatial_proximity.max() - spatial_proximity.min()) \n",
    "    #normalize\n",
    "    \n",
    "    dti_avg = pd.read_csv(PATH + \"/dti_998avg.csv\", header=None)\n",
    "    dti_avg = dti_avg.to_numpy().flatten()\n",
    "    dti_mat_avg = flattened_to_matrix(dti_avg)\n",
    "    dti = transpose_add_flatten(dti_mat_avg)\n",
    "        \n",
    "    structures = read_structure(PATH + \"/structures.csv\", 0)\n",
    "    dti_copy = copy.deepcopy(dti)\n",
    "\n",
    "    if scaling_params[\"weigh_topo\"] == True: \n",
    "        # apply log scale on weight to compute topological similarity\n",
    "        dti_copy = dti_copy + 1e-09\n",
    "        dti_copy = np.log(dti_copy)\n",
    "        dti_copy = (dti_copy - dti_copy.min()) / (dti_copy.max() - dti_copy.min())\n",
    "    \n",
    "    max_scaleKnown = 1 / 10**(min(scaling_params[\"scales_known\"]))\n",
    "    min_scaleKnown = 1 / 10**(max(scaling_params[\"scales_known\"]))\n",
    "    max_scaleToPredict = 1 / 10**(min(scaling_params[\"scales_toPredict\"]))\n",
    "    min_scaleToPredict = 1 / 10**(max(scaling_params[\"scales_toPredict\"]))\n",
    "    if max(scaling_params[\"scales_toPredict\"]) == 8:\n",
    "        min_scaleToPredict = 0\n",
    "    \n",
    "    max_scaleKnown_c = -np.floor(np.log10(max_scaleKnown)).astype(int)\n",
    "    min_scaleKnown_c = -np.floor(np.log10(min_scaleKnown)).astype(int)\n",
    "    max_scaleToPredict_c = -np.floor(np.log10(max_scaleToPredict)).astype(int)\n",
    "    min_scaleToPredict_c = -1\n",
    "    if min_scaleToPredict == 0:\n",
    "        min_scaleToPredict_c = 8\n",
    "    else:\n",
    "        min_scaleToPredict_c = -np.floor(np.log10(min_scaleToPredict)).astype(int)\n",
    "        \n",
    "    ############# COMPUTE TOPO\n",
    "    \n",
    "    # Turn dti_copy to binary array with 1 for known links and 0 for unknown\n",
    "    dti_copy = np.where(dti_copy <= 10 * max_scaleKnown, dti_copy, 0)\n",
    "    dti_copy = np.where(dti_copy >= min_scaleKnown, dti_copy, 0)\n",
    "    if scaling_params[\"weigh_topo\"] == False:\n",
    "        dti_copy = np.where(dti_copy > 0, 1, 0)\n",
    "\n",
    "    topo_arr = backboneDTI_to_topo(dti_copy)\n",
    "    topo_arr = topo_arr.flatten()\n",
    "    ####\n",
    "    X_t = np.column_stack((spatial_proximity, structures))\n",
    "    X_t = np.column_stack((X_t, topo_arr))\n",
    "    \n",
    "    ############# COMPUTE TOPO, DONE\n",
    "    \n",
    "    ############# PRUNE AND LEAVE ONLY LINKS TO PREDICT\n",
    "    \n",
    "    print(\"ymax_beforeRemovingStrongLinks_wholeBrain: \" + str(dti.max()))\n",
    "#     print(\"x\")\n",
    "#     dti_copy = np.delete(dti_copy, idx_removed_byCoverage, 0)\n",
    "#     print(\"ymax_beforeRemovingStrongLinks: \" + str(dti.max()))\n",
    "\n",
    "    idx_exceptToPredict = []\n",
    "    for i in range(len(dti)):\n",
    "        if (dti[i] < max_scaleToPredict*10 and dti[i] >= min_scaleToPredict) == False:\n",
    "            idx_exceptToPredict.append(i)\n",
    "    print(len(idx_exceptToPredict))\n",
    "    \n",
    "    idx_removed_byCoverage = removeByCoverage(scaling_params[\"coverage\"])\n",
    "    print(len(idx_removed_byCoverage))\n",
    "    \n",
    "    idx_removed = idx_removed_byCoverage + idx_exceptToPredict\n",
    "    \n",
    "    Y = np.delete(dti, idx_removed, 0)\n",
    "    X = np.delete(X_t, idx_removed, 0)\n",
    "    \n",
    "    ############# PRUNE AND LEAVE ONLY LINKS TO PREDICT, DONE \n",
    "\n",
    "    Y_categorical = copy.deepcopy(Y)\n",
    "    Y_categorical += 1e-11\n",
    "\n",
    "    # miss\n",
    "    Y_categorical = np.where(Y_categorical > 7, 8, Y_categorical) # if 0 in original dti, convert to category 8\n",
    "    \n",
    "    ############# COLLECT INDICES\n",
    "    \n",
    "    num_scalesToPredict = min_scaleToPredict_c - max_scaleToPredict_c + 1\n",
    "    print(\"num_scalesToPredict: \" + str(num_scalesToPredict))\n",
    "    \n",
    "    categorical_indices = []\n",
    "    for i in range(num_scalesToPredict):\n",
    "        categorical_indices.append([])\n",
    "#     print(categorical_indices)\n",
    "    \n",
    "    for i in range(len(Y_categorical)):\n",
    "        idx_1 = Y_categorical[i] - max_scaleToPredict_c # index in indices array\n",
    "        categorical_indices[idx_1].append(i)\n",
    "        \n",
    "    for i in range(num_scalesToPredict):\n",
    "        print(np.shape(categorical_indices[i]))\n",
    "\n",
    "    ############# COLLECT INDICES, DONE\n",
    "    \n",
    "    ############# K-FOLD\n",
    "    \n",
    "    K = 5\n",
    "    kfold_indices = []\n",
    "    for i in range(num_scalesToPredict):\n",
    "        count_curScale = len(categorical_indices[i])\n",
    "        fold_size = int(count_curScale/K) + 1\n",
    "        kfold_indices.append(chunks(categorical_indices[i], fold_size))\n",
    "#         print(count_curScale)\n",
    "        \n",
    "#     for i in range(num_scalesToPredict):\n",
    "#         for j in range(K):\n",
    "#             print(np.shape(kfold_indices[i][j]))\n",
    "    \n",
    "    # return lsp\n",
    "    \n",
    "    kfold_indices_rearranged = []\n",
    "    for i in range(K):\n",
    "        kfold_indices_rearranged.append([])\n",
    "        \n",
    "    for i in range(num_scalesToPredict):\n",
    "        for j in range(K):\n",
    "            kfold_indices_rearranged[j].append(kfold_indices[i][j])\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(num_scalesToPredict):\n",
    "            print(np.shape(kfold_indices_rearranged[i][j]))\n",
    "    \n",
    "    # return \n",
    "\n",
    "    kfold_flattened = []\n",
    "    \n",
    "    for i in range(K):\n",
    "        fold = kfold_indices_rearranged[i]\n",
    "        flat_fold = [x for sublist in fold for x in sublist]\n",
    "        kfold_flattened.append(flat_fold)\n",
    "    \n",
    "    print(\"shape: kfold_flattened\")\n",
    "    for i in range(K):\n",
    "        print(np.shape(kfold_flattened[i]))\n",
    "    \n",
    "    ############# K-FOLD, DONE\n",
    "\n",
    "    axis = 0\n",
    "    print(\"fold shapes\")\n",
    "    \n",
    "    X_folds = []\n",
    "    Y_folds = []\n",
    "    Y_categorical_folds = []\n",
    "    \n",
    "    for i in range(K):\n",
    "        fold_indices = kfold_flattened[i]\n",
    "        X_folds.append(np.take(X, fold_indices, axis))\n",
    "        Y_folds.append(np.take(Y, fold_indices, axis))\n",
    "        Y_categorical_folds.append(np.take(Y_categorical, fold_indices, axis))\n",
    "        print(np.shape(X_folds[i]))\n",
    "    \n",
    "    ############# SCALING\n",
    "    for i in range(K):\n",
    "        Y_folds[i] = scaling(Y_folds[i], scaling_params[\"apply_log\"], scaling_params[\"beta\"])\n",
    "    ############# SCALING ,DONE\n",
    "    \n",
    "    print(\"shapes: X: \" + str(np.shape(X)) + \"   Y : \" + str(np.shape(Y)))\n",
    "\n",
    "#     binary array, 1 for weak links known\n",
    "#     Correct answers\n",
    "    \n",
    "    output_folder = PATH\n",
    "    output_folder += \"/AvgData/\"\n",
    "    output_folder += process_folderName(scaling_params)\n",
    "    print(output_folder)\n",
    "\n",
    "    if path_dict[\"isTrain\"] == True:\n",
    "        output_folder += \"/Train\"\n",
    "    else:\n",
    "        output_folder += \"/Test\"\n",
    "    print(output_folder)\n",
    "    \n",
    "    x_path = output_folder + \"/X/X_\"\n",
    "    y_prob_path = output_folder + \"/Y/Y_prob_\"\n",
    "    y_categorical_path = output_folder + \"/Y/Y_categorical_\"\n",
    "    x_path += \"avg\"\n",
    "    y_prob_path += \"avg\"\n",
    "    y_categorical_path += \"avg\"\n",
    "\n",
    "    print(x_path)\n",
    "    print(y_prob_path)\n",
    "    print(y_categorical_path)\n",
    "    \n",
    "    for i in range(K):\n",
    "        x = np.savetxt(x_path + \"-f\" + str(i+1) + \".csv\", X_folds[i], delimiter=\",\",fmt = \"%s\")\n",
    "        y = np.savetxt(y_prob_path + \"-f\" + str(i+1) + \".csv\", Y_folds[i], delimiter=\",\",fmt = \"%s\")\n",
    "        y_categorical = np.savetxt(y_categorical_path+ \"-f\" + str(i+1) + \".csv\", Y_categorical_folds[i], \n",
    "                                   delimiter=\",\",fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcdf22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv_dataset(sub_idx, subid_list, path_dict, scaling_params):\n",
    "    if scaling_params[\"isGraph\"]:\n",
    "        to_csv_datataset_graph(sub_idx, subid_list, path_dict, scaling_params)\n",
    "        return\n",
    "    \n",
    "    subject_ID = subid_list[sub_idx]\n",
    "    PATH = path_dict[\"PATH\"]\n",
    "#     print(\"haha\")\n",
    "    \n",
    "    distance_matrix = pd.read_csv(path_dict[\"distance_path\"], header=None)\n",
    "\n",
    "    dist_arr = distance_matrix.to_numpy()\n",
    "    spatial_proximity = dist_arr.flatten()\n",
    "    \n",
    "    #################################### SHOULD NORMALIZE LATER !!!!!!!!!!!!! SUBJECT TO CHANGES\n",
    "    spatial_proximity = (spatial_proximity - spatial_proximity.min()) / (spatial_proximity.max() - spatial_proximity.min()) \n",
    "    #normalize\n",
    "#     print(\"lc\")\n",
    "    \n",
    "    dti = pd.read_csv(PATH + \"/dti_csv/dti_mat/\" + str(subject_ID) + \"_MMP_matrix.csv\", header=None)\n",
    "    dti_np = dti.to_numpy()\n",
    "    dti = transpose_add_flatten(dti_np)\n",
    "    print(np.shape(dti))\n",
    "#     print(\"dt\")\n",
    "\n",
    "    structure_csv = \"localStructures_\" + str(subject_ID) + \".csv\"\n",
    "    structures = read_structure(PATH + \"/structures_csv/\" + structure_csv, 0)\n",
    "    \n",
    "#     if scaling_params[\"isAverage\"]:\n",
    "#         dti_avg = pd.read_csv(PATH + \"/dti_998avg.csv\", header=None)\n",
    "#         dti_avg = dti_avg.to_numpy().flatten()\n",
    "#         dti_mat_avg = flattened_to_matrix(dti_avg)\n",
    "#         dti = transpose_add_flatten(dti_mat_avg)\n",
    "        \n",
    "#         structures = read_structure(PATH + \"/structures.csv\", 0)\n",
    "#         print(\"avg!\")\n",
    "\n",
    "    dti_copy = copy.deepcopy(dti)\n",
    "\n",
    "    if scaling_params[\"weigh_topo\"] == True: \n",
    "        # apply log scale on weight to compute topological similarity\n",
    "        dti_copy = dti_copy + 1e-09\n",
    "        dti_copy = np.log(dti_copy)\n",
    "        dti_copy = (dti_copy - dti_copy.min()) / (dti_copy.max() - dti_copy.min())\n",
    "    \n",
    "    max_scaleKnown = 1 / 10**(min(scaling_params[\"scales_known\"]))\n",
    "    min_scaleKnown = 1 / 10**(max(scaling_params[\"scales_known\"]))\n",
    "    max_scaleToPredict = 1 / 10**(min(scaling_params[\"scales_toPredict\"]))\n",
    "    min_scaleToPredict = 1 / 10**(max(scaling_params[\"scales_toPredict\"]))\n",
    "    if max(scaling_params[\"scales_toPredict\"]) == 8:\n",
    "        min_scaleToPredict = 0\n",
    "    \n",
    "    # Turn dti_copy to binary array with 1 for known links and 0 for unknown\n",
    "    dti_copy = np.where(dti_copy <= 10 * max_scaleKnown, dti_copy, 0)\n",
    "    dti_copy = np.where(dti_copy >= min_scaleKnown, dti_copy, 0)\n",
    "    if scaling_params[\"weigh_topo\"] == False:\n",
    "        dti_copy = np.where(dti_copy > 0, 1, 0)\n",
    "\n",
    "    topo_arr = backboneDTI_to_topo(dti_copy)\n",
    "    topo_arr = topo_arr.flatten()\n",
    "    ####\n",
    "    X_t = np.column_stack((spatial_proximity, structures))\n",
    "    X_t = np.column_stack((X_t, topo_arr))\n",
    "    \n",
    "    print(\"ymax_beforeRemovingStrongLinks_wholeBrain: \" + str(dti.max()))\n",
    "    \n",
    "#     dti_copy = np.delete(dti_copy, idx_removed_byCoverage, 0)\n",
    "#     print(\"ymax_beforeRemovingStrongLinks: \" + str(dti.max()))\n",
    "\n",
    "    idx_exceptToPredict = []\n",
    "    for i in range(len(dti)):\n",
    "        if (dti[i] < max_scaleToPredict*10 and dti[i] >= min_scaleToPredict) == False:\n",
    "            idx_exceptToPredict.append(i)\n",
    "    print(len(idx_exceptToPredict))\n",
    "    \n",
    "    idx_removed_byCoverage = removeByCoverage(scaling_params[\"coverage\"])\n",
    "    print(len(idx_removed_byCoverage))\n",
    "    \n",
    "    \n",
    "    idx_removed = idx_removed_byCoverage + idx_exceptToPredict\n",
    "    \n",
    "    Y = np.delete(dti, idx_removed, 0)\n",
    "    X = np.delete(X_t, idx_removed, 0)\n",
    "    #this\n",
    "\n",
    "    Y_categorical = copy.deepcopy(Y)\n",
    "    Y_categorical+= 1e-11\n",
    "    Y_categorical = -np.floor(np.log10(Y_categorical)).astype(int)\n",
    "    Y_categorical = np.where(Y_categorical > 7, 8, Y_categorical) # if 0 in original dti\n",
    "    \n",
    "    if scaling_params[\"apply_log\"] == False:\n",
    "        Y = Y ** scaling_params[\"beta\"]\n",
    "        print(\"beta:  \" +  str(scaling_params[\"beta\"])   + \"Y_max_beta: \" + str(Y.max()))\n",
    "    else:\n",
    "        Y = Y + 1e-09\n",
    "        Y = np.log(Y)\n",
    "        print(\"log:  \" + \"Y_max_log: \" + str(Y.max()))\n",
    "    \n",
    "    Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "    print(\"shapes: X: \" + str(np.shape(X)) + \"   Y : \" + str(np.shape(Y)))\n",
    "    \n",
    "#     binary array, 1 for weak links known\n",
    "#     Correct answers\n",
    "    \n",
    "    output_folder = PATH\n",
    "    if scaling_params[\"isAverage\"]:\n",
    "        output_folder += \"/AvgData/\"\n",
    "    else:\n",
    "        output_folder += \"/Data/\"\n",
    "    \n",
    "    output_folder += process_folderName(scaling_params)\n",
    "    print(output_folder)\n",
    "\n",
    "    if path_dict[\"isTrain\"] == True:\n",
    "        output_folder += \"/Train\"\n",
    "    else:\n",
    "        output_folder += \"/Test\"\n",
    "#     output_folder += \"/Train\"\n",
    "    print(output_folder)\n",
    "    \n",
    "    x_path = output_folder + \"/X/X_\"\n",
    "    y_prob_path = output_folder + \"/Y/Y_prob_\"\n",
    "    y_categorical_path = output_folder + \"/Y/Y_categorical_\"\n",
    "    \n",
    "    if scaling_params[\"isAverage\"]:\n",
    "        x_path += \"avg.csv\"\n",
    "        y_prob_path += \"avg.csv\"\n",
    "        y_categorical_path += \"avg.csv\"\n",
    "    else:\n",
    "        x_path += str(subject_ID) + \".csv\"\n",
    "        y_prob_path += str(subject_ID) + \".csv\"\n",
    "        y_categorical_path += str(subject_ID) + \".csv\"\n",
    "    \n",
    "    print(\"+1\")\n",
    "    print(x_path)\n",
    "    print(y_prob_path)\n",
    "    print(y_categorical_path)\n",
    "    x = np.savetxt(x_path, X, delimiter=\",\",fmt = \"%s\")\n",
    "    y = np.savetxt(y_prob_path, Y, delimiter=\",\",fmt = \"%s\")\n",
    "    y_categorical = np.savetxt(y_categorical_path, Y_categorical, delimiter=\",\",fmt = \"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c005789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(folder, cur_dir):\n",
    "    subfolders = ['X', 'Y', 'Y']\n",
    "    prefix = ['X', 'Y_prob','Y_categorical']\n",
    "    \n",
    "    for i in range(3):\n",
    "        x_path = cur_dir + \"/\" + folder + \"/\" + subfolders[i] + \"/\"\n",
    "        all_x_files = glob.glob(os.path.join(x_path, prefix[i] + \"_*.csv\"))\n",
    "        sorted_x_files = sorted(all_x_files)\n",
    "#         print(sorted_x_files)\n",
    "\n",
    "        for x_file in sorted_x_files:\n",
    "            sub_id = int(''.join(filter(str.isdigit, x_file)))\n",
    "#             print(sub_id)\n",
    "\n",
    "        df_from_each_file = (pd.read_csv(f, sep=',',header=None) for f in sorted_x_files)\n",
    "        df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
    "        saved_arr = df_merged.to_numpy()\n",
    "        merged = np.savetxt(cur_dir + \"/\" + folder + \"/\" + prefix[i] + \n",
    "                            \".csv\", saved_arr, delimiter=\",\",fmt = \"%s\")\n",
    "        \n",
    "#         df_merged.to_csv(cur_dir + \"/\" + folder + \"/\" + subfolders[i] + \"_\" + folder + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0172a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataFolder(folder_name):\n",
    "    folder = cur_dir + folder_name\n",
    "    train_path = folder + \"/Train\"\n",
    "    test_path = folder + \"/Test\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "        os.mkdir(train_path)\n",
    "        os.mkdir(train_path + \"/X\")\n",
    "        os.mkdir(train_path + \"/Y\")\n",
    "        os.mkdir(test_path)\n",
    "        os.mkdir(test_path + \"/X\")\n",
    "        os.mkdir(test_path + \"/Y\")\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % folder_name)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620bbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folderName(scaling_params):\n",
    "    folder_name = scaling_params[\"coverage\"] + \"_\"\n",
    "    scales_known = scaling_params[\"scales_known\"]\n",
    "    scales_toPredict = scaling_params[\"scales_toPredict\"]\n",
    "    \n",
    "    for i in scales_known:\n",
    "        folder_name += str(i)\n",
    "    folder_name += \"to\"\n",
    "    for j in scales_toPredict:\n",
    "        folder_name += str(j)\n",
    "    folder_name += \"_\"\n",
    "    \n",
    "    if scaling_params[\"apply_log\"] == True:\n",
    "        folder_name += \"Log\"\n",
    "    else:\n",
    "        folder_name += \"beta\" + str(scaling_params[\"beta\"])\n",
    "        \n",
    "    if scaling_params[\"weigh_topo\"] == True:\n",
    "        folder_name += \"_wcm\" \n",
    "    \n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad59260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(subid_list, path_dict, scaling_params, num_individuals):\n",
    "    new_foldername = process_folderName(scaling_params)\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    if scaling_params[\"isAverage\"]:\n",
    "        if not scaling_params[\"isGraph\"]:\n",
    "            create_dataFolder(\"/AvgData/\" + new_foldername)\n",
    "            to_csv_dataset_avg(path_dict, scaling_params)\n",
    "            return\n",
    "        else:\n",
    "            create_dataFolder(\"/GraphAvgData/\" + new_foldername)\n",
    "            to_csv_dataset(0, subid_list, path_dict, scaling_params)\n",
    "            return\n",
    "    \n",
    "    if scaling_params[\"isGraph\"]:\n",
    "        if not scaling_params[\"isAverage\"]:\n",
    "            create_dataFolder(\"/GraphData/\" + new_foldername)\n",
    "    else:\n",
    "        create_dataFolder(\"/Data/\" + new_foldername)\n",
    "    \n",
    "    for sub_idx in range(num_individuals):\n",
    "        if sub_idx % 10 != 0:\n",
    "            path_dict[\"isTrain\"] = True\n",
    "            to_csv_dataset(sub_idx, subid_list, path_dict, scaling_params)\n",
    "        else:\n",
    "            path_dict[\"isTrain\"] = False\n",
    "            to_csv_dataset(sub_idx, subid_list, path_dict, scaling_params)\n",
    "            \n",
    "        print(\"Individual No.\" + str(int(sub_idx + 1)))\n",
    "        end = timer()\n",
    "        print(timedelta(seconds=end-start))\n",
    "    \n",
    "    if not scaling_params[\"isGraph\"]:\n",
    "        merge_csv('Data/' + new_foldername + \"/Train\", cur_dir)\n",
    "        print(\"Done merging\")\n",
    "#     merge_csv('Data/' + new_foldername + \"/Test\", cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e128c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
